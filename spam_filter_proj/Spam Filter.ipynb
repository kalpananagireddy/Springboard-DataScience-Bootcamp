{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Youtube is one of the most popular video sharing platform with more than 1 billion users. Users have long been outraged by the overwhelming number of spam messages in the comment section. In 2012 users created a petition asking Youtube to provide tools to deal with undesired content. In 2013, spam problem gets worse as Google overhauled the YouTube comment system to connect it to Google+, which allows users to post links. This attracts more malicious users to self-promote their videos using the platform. This project will build a spam filter to automatically filter spam comments.\n",
    "\n",
    "\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "In this part we will:\n",
    "\n",
    "    1: load the data into notebook\n",
    "    2: check for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=pd.read_csv('./data/Youtube01-Psy.csv',encoding = 'utf-8-sig')\n",
    "s2=pd.read_csv('./data/Youtube02-KatyPerry.csv',encoding = 'utf-8-sig')\n",
    "s3=pd.read_csv('./data/Youtube03-LMFAO.csv',encoding = 'utf-8-sig')\n",
    "s4=pd.read_csv('./data/Youtube04-Eminem.csv',encoding = 'utf-8-sig')\n",
    "s5=pd.read_csv('./data/Youtube05-Shakira.csv',encoding = 'utf-8-sig')\n",
    "s1['song']='Psy'\n",
    "s2['song']='KetyPerry'\n",
    "s3['song']='LMFAO'\n",
    "s4['song']='Eminem'\n",
    "s5['song']='Shakira'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 6 columns):\n",
      "COMMENT_ID    1956 non-null object\n",
      "AUTHOR        1956 non-null object\n",
      "DATE          1711 non-null object\n",
      "CONTENT       1956 non-null object\n",
      "CLASS         1956 non-null int64\n",
      "song          1956 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 107.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df=pd.concat([s1,s2,s3,s4,s5])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Psy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>Psy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>Psy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>Psy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>Psy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS song  \n",
       "0      1  Psy  \n",
       "1      1  Psy  \n",
       "2      1  Psy  \n",
       "3      1  Psy  \n",
       "4      1  Psy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0     951\n",
       "1    1005\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('CLASS').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are total of 1956 comments in this data for the 5 most popular songs. 951 hams and 1005 spams. There are some missing values in the column of date, but we will focus mostly on the CONTENT column so this will not affect us. The CLASS section is the label: 1 means spam and 0 means ham. \n",
    "\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "We start the data analysis by asking some basic questions on the dataset:\n",
    "\n",
    "    *is the average length of comment different among ham and spam?\n",
    "    *can we identify some suspicious user account that seems relate to spam?\n",
    "    *are spam comments more likely to have a URL in them?\n",
    "    *does the spam comments have any correlation with time? (if it's fake account, maybe it's setup to send out spam comments periodically.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the length of the comment.\n",
    "\n",
    "test= df['CONTENT'].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first expand contractions, so aren't will be are not so it's counted as two words.\n",
    "\n",
    "import contractions\n",
    "\n",
    "def contraction_expand(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "df['CONTENT']=df['CONTENT'].apply(contraction_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.827550</td>\n",
       "      <td>56.526731</td>\n",
       "      <td>2</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.769154</td>\n",
       "      <td>159.459172</td>\n",
       "      <td>10</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean         std  min   max\n",
       "CLASS                                   \n",
       "0       49.827550   56.526731    2   755\n",
       "1      137.769154  159.459172   10  1200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#tokenize workds\n",
    "\n",
    "import nltk\n",
    "df['words']=df['CONTENT'].apply(nltk.word_tokenize)\n",
    "\n",
    "#calculate sentence length\n",
    "\n",
    "df['length']=df['CONTENT'].apply(len)\n",
    "\n",
    "df.groupby('CLASS')['length'].agg(['mean','std','min','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average length of spam vs ham comments are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10071574642126789"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if there contains URL, let's also include youtube URL (need to look up for it)\n",
    "\n",
    "import re\n",
    "\n",
    "def URL (string):\n",
    "    pattern=r'http[s]?://*'\n",
    "    return (bool(re.search(pattern,string)))\n",
    "\n",
    "df['URL']=df['CONTENT'].apply(URL)\n",
    "sum(df['URL'])/len(df['URL'])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0     11.0\n",
       "1    186.0\n",
       "Name: URL, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.groupby('CLASS')['URL'].agg(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 10% of the comments contains URL, and most URLs are found in spam comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.017857\n",
       "1    0.119760\n",
       "2    0.026316\n",
       "3    0.000000\n",
       "4    0.153846\n",
       "Name: capital, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate percentage of capital letters in a comment\n",
    "\n",
    "def capital_letters(string):\n",
    "    return (len(re.findall('[A-Z]',string))/len(string))\n",
    "\n",
    "df['capital']=df['CONTENT'].apply(capital_letters)\n",
    "df['capital'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090289</td>\n",
       "      <td>0.161128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.108451</td>\n",
       "      <td>0.173301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean       std  min       max\n",
       "CLASS                                   \n",
       "0      0.090289  0.161128  0.0  1.000000\n",
       "1      0.108451  0.173301  0.0  0.919355"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('CLASS')['capital'].agg(['mean','std','min','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform a significant test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS  AUTHOR               \n",
       "0      5000palo                 7\n",
       "       Marshmallow Kingdom      3\n",
       "       Seth Ryan                3\n",
       "       Alain Bruno              2\n",
       "       Athena Gomez             2\n",
       "       BigBird Larry            2\n",
       "       Brian Brai               2\n",
       "       Chris Madzier            2\n",
       "       D Maw                    2\n",
       "       Eric Gonzalez            2\n",
       "       Juan Martinez            2\n",
       "       LaiLa Steudle            2\n",
       "       LiveLikeLien x           2\n",
       "       Naga Berapi              2\n",
       "       Paul Crowder             2\n",
       "       Pepe The Meme King       2\n",
       "       Sonny Carter             2\n",
       "       The Technology Zoo       2\n",
       "       Warcorpse666             2\n",
       "       janet rangel             2\n",
       "       lol Ippocastano          2\n",
       "       tyler sleetway           2\n",
       "          Berty  Winata         1\n",
       "       Aarjav Parmar            1\n",
       "       Abdinasir Omar           1\n",
       "       Abdou Abdou              1\n",
       "       Abdullah Alawani         1\n",
       "       Abhi Vats                1\n",
       "       Abhishek Kumar Gendah    1\n",
       "       Abigail Gustav           1\n",
       "                               ..\n",
       "1      tchangkou                1\n",
       "       the34104                 1\n",
       "       themagicmangotree        1\n",
       "       themind blasters         1\n",
       "       ultimatecleanmusic       1\n",
       "       vinh nguyen              1\n",
       "       volleyballspikerable     1\n",
       "       xXxPWND 420xXx           1\n",
       "       xiaoying chen            1\n",
       "       yakikukamo FIRELOVER     1\n",
       "       zakaia ziko              1\n",
       "       zeynep saka              1\n",
       "       zonZi                    1\n",
       "       ||GuitarZ||              1\n",
       "       Æle Skittles             1\n",
       "       Александр Федоров        1\n",
       "       Михаил Панкратов         1\n",
       "       Никита Безухов           1\n",
       "       Олег Пась                1\n",
       "       Олег Хижняк              1\n",
       "       алексей алгаев           1\n",
       "       احمد الهوارى             1\n",
       "       ‫הילה שוהם‬‎             1\n",
       "       ‫انس الغامدي‬‎           1\n",
       "       ‫جوجو جوجو‬‎             1\n",
       "       ‫حلم الشباب‬‎            1\n",
       "       ‫غاردينا خالد‬‎          1\n",
       "       이 정훈                     1\n",
       "       조윤기                      1\n",
       "       ＯＧＶＡＤＥＲ                  1\n",
       "Name: AUTHOR, Length: 1793, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('CLASS')['AUTHOR'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1793 unique user names in this dataset. There is a small fraction of users who make multiple comments but all of users who created spams only combment once. It's noticed that some spam accounts are from foreign countries (since their names are not English character). Next we will see how many foreign user names for spam and ham group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS  user_isEnglish\n",
       "0      True              901\n",
       "       False              50\n",
       "1      True              956\n",
       "       False              49\n",
       "Name: user_isEnglish, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many of non_english characters in the username\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "df['user_isEnglish']=df['AUTHOR'].apply(isEnglish)\n",
    "df.groupby('CLASS')['user_isEnglish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90                               Никита Безухов\n",
       "91                             Михаил Панкратов\n",
       "95                                    Олег Пась\n",
       "175                                 David Boček\n",
       "192                              Uroš Slemenjak\n",
       "210                          O sábio das 7 eras\n",
       "333                           Александр Федоров\n",
       "337                               Tofik Miedzyń\n",
       "20                                Cléda Dimitri\n",
       "40                                   Mai Nguyễn\n",
       "80                                 Nicolás Jara\n",
       "83                                Mättr Valleni\n",
       "179                     Mehmet Ertuğrul Tohumcu\n",
       "248                                احمد الهوارى\n",
       "282                            Nedim Alp SEÇGEL\n",
       "302                          Quinho Divulgaçoes\n",
       "308                              Uroš Slemenjak\n",
       "175    TelePricol - FUNNY VIDEOS,ЛУЧШИЕ ПРИКОЛЫ\n",
       "205                     Synchronized™ Nightcore\n",
       "210             Almohtarif Info | المحترف انفو‎\n",
       "227                                NerdyQueen〈3\n",
       "236                                ‫הילה שוהם‬‎\n",
       "256                                VidéoRiveaux\n",
       "260                                 CHM Guitar™\n",
       "263                                         조윤기\n",
       "272                          Villads Hallundbæk\n",
       "283                                 Олег Хижняк\n",
       "285                               Jephone Cañas\n",
       "300                                ‫جوجو جوجو‬‎\n",
       "312                              алексей алгаев\n",
       "334                             ‫غاردينا خالد‬‎\n",
       "336                              ‫مريم الهندي‬‎\n",
       "337                              ‫مريم الهندي‬‎\n",
       "358                                Janja Kmetič\n",
       "393                                     ＯＧＶＡＤＥＲ\n",
       "403                       Kikilaronk “Kiki” Loz\n",
       "404                                   Dávvvid ツ\n",
       "24                          Martin Gébele MUSIC\n",
       "214                                Æle Skittles\n",
       "269                                        이 정훈\n",
       "287                                 Noise​Break\n",
       "322                              Greg Fils Aimé\n",
       "36                                ‫حلم الشباب‬‎\n",
       "154                              ‫انس الغامدي‬‎\n",
       "254                                 Noise​Break\n",
       "258                              Greg Fils Aimé\n",
       "277                    Krystian Konrad Moreński\n",
       "278                    Krystian Konrad Moreński\n",
       "317                                Stanša Matej\n",
       "Name: AUTHOR, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CLASS']==1) & (df['user_isEnglish']==False)]['AUTHOR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much suspecious account we can see by looking at ethenity groups from user names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour from time data\n",
    "# explore the distribution of # of spam/ham among hours\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning model\n",
    "\n",
    "In this section we are going to build classifiers to filter out spam. \n",
    "\n",
    "As a first step, let's start with something simple. Let's create a bag of words and use naive-bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the confusion matrix: \n",
      " [[221  29]\n",
      " [ 11 228]]\n",
      "the accuracy score is  0.918200408997955\n"
     ]
    }
   ],
   "source": [
    "# create a bag of words\n",
    "# train with naive bayes\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import *\n",
    "\n",
    "X=df.drop(['CLASS'],axis=1)\n",
    "y=df['CLASS']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "pl=Pipeline([('vec',CountVectorizer()),('clf',MultinomialNB())])\n",
    "pl.fit(X_train['CONTENT'],y_train)\n",
    "print(\"the confusion matrix: \\n\", confusion_matrix(y_test,pl.predict(X_test['CONTENT'])))\n",
    "print(\"the accuracy score is \", accuracy_score(y_test,pl.predict(X_test['CONTENT'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logistic regression\n",
    "# use cross validation to tune hypoparameters\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words will create a lot of features and we know that SVM works good with high dimensions. Let's try SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we explore if more features are added\n",
    "# add length of the comment\n",
    "# add if it contain URL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
